{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('lab3').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|  _c0|  _c1|         _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "|53719|60579|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.option(\"recursiveFileLookup\", \"true\").csv(\"./donation\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, lower, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+----------+------+---------------+\n",
      "|id_1 |id_2 |cmp_fname_c1     |cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|clean_text|tokens|filtered_tokens|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+----------+------+---------------+\n",
      "|37291|53113|0.833333333333333|null        |1.0         |null        |1      |1     |1     |1     |0      |true    |1         |[1]   |[1]            |\n",
      "|39086|47614|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|70031|70237|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|84795|97439|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|36950|42116|1.0              |null        |1.0         |1.0         |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|42413|48491|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|25965|64753|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|49451|90407|1.0              |null        |1.0         |null        |1      |1     |1     |1     |0      |true    |1         |[1]   |[1]            |\n",
      "|39932|40902|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|46626|47940|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|48948|98379|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|4767 |4826 |1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|45463|69659|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|11367|13169|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|10782|89636|1.0              |null        |1.0         |null        |1      |0     |1     |1     |1      |true    |0         |[0]   |[0]            |\n",
      "|26206|39147|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|16662|27083|1.0              |1.0         |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|18823|30204|1.0              |1.0         |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|38545|85418|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "|28963|39172|1.0              |null        |1.0         |null        |1      |1     |1     |1     |1      |true    |1         |[1]   |[1]            |\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+----------+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n",
    "option(\"inferSchema\", \"true\").csv(\"donation/block_1.csv\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.withColumn(\"clean_text\", lower(col(\"cmp_bd\")))\n",
    "    df = df.withColumn(\"clean_text\", regexp_replace(col(\"clean_text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "    tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"tokens\")\n",
    "    df = tokenizer.transform(df)\n",
    "    remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "    df = remover.transform(df)\n",
    "    return df\n",
    "preprocessed_df2 = preprocess_data(df2)\n",
    "preprocessed_df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|is_match| count|\n",
      "+--------+------+\n",
      "|   false|572820|\n",
      "|    true|  2093|\n",
      "+--------+------+\n",
      "\n",
      "+--------+------+\n",
      "|is_match|   cnt|\n",
      "+--------+------+\n",
      "|   false|572820|\n",
      "|    true|  2093|\n",
      "+--------+------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|      cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+------------------+------------------+\n",
      "|  count|            574811|             10325|\n",
      "|   mean|0.7127592938253411|0.8977586763518969|\n",
      "| stddev|0.3889286452463531|0.2742577520430532|\n",
      "|    min|               0.0|               0.0|\n",
      "|    max|               1.0|               1.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n",
    "df2.createOrReplaceTempView(\"linkage\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT is_match, COUNT(*) cnt\n",
    "    FROM linkage\n",
    "    GROUP BY is_match\n",
    "    ORDER BY cnt DESC\n",
    "\"\"\").show()\n",
    "\n",
    "summary=df2.describe()\n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n",
    "matches=df2.where(\"is_match = true\")\n",
    "match_summary=matches.describe()\n",
    "misses=df2.filter(col(\"is_match\")==False)\n",
    "miss_summary=misses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/lplab/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/lplab/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lplab/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/lplab/.local/lib/python3.10/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'set_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-402c8db790eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdescT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmatch_summaryT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpivot_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmiss_summaryT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpivot_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiss_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgood_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cmp_lname_c1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cmp_plz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cmp_by\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cmp_bd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cmp_bm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-402c8db790eb>\u001b[0m in \u001b[0;36mpivot_summary\u001b[0;34m(desc)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpivot_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdesc_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdesc_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdesc_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'field'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdesc_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'set_index'"
     ]
    }
   ],
   "source": [
    "# import decs\n",
    "def pivot_summary(desc):\n",
    "    desc_p=desc.toPandas\n",
    "    desc_p=desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p=desc_p.rename(columns={'index':'field'})\n",
    "    desc_p=desc_p.rename_axis(None, axis=1)\n",
    "    descT=spark.createDataFrame(desc_p)\n",
    "    for c in descT.columns:\n",
    "        if c=='field':\n",
    "            continue\n",
    "        else:\n",
    "            descT=descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n",
    "\n",
    "match_summaryT=pivot_summary(match_summary)\n",
    "miss_summaryT=pivot_summary(miss_summary)\n",
    "good_features=[\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "# sum_expression=\" + \".join(good_features)\n",
    "\n",
    "scored=df2.fillna(0, subset=good_features).\\\n",
    "                withColumn('score', expr(sum_expression)).\\\n",
    "                select('score', 'is_match')\n",
    "\n",
    "scored.show()\n",
    "\n",
    "def crossTabs(scored: DataFrame, t: float) -> DataFrame:\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n",
    "        groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n",
    "        count()\n",
    "\n",
    "confused=crossTabs(scored, 4.0)\n",
    "confused.show()\n",
    "confused2=crossTabs(scored, 2.0)\n",
    "confused2.show()\n",
    "\n",
    "tp=confused.filter(\"above = true\").select(\"true\").collect()[0].true\n",
    "fp=confused.filter(\"above = true\").select(\"false\").collect()[0].false\n",
    "fn=confused.filter(\"above = false\").select(\"true\").fillna(0).collect()[0].true\n",
    "tn=confused.filter(\"above = false\").select(\"false\").collect()[0].false\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
